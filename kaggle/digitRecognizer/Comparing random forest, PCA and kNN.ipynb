{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7a71f50-2d95-5715-7c97-8e4baf788bfb",
    "_uuid": "d1b03912437e8b8a39e733d0266a97849348a60e"
   },
   "source": [
    "# Analysis of the MNIST dataset\n",
    "\n",
    "\n",
    "\n",
    "In this analysis we will apply the following methods to the MNIST classification problem:\n",
    "\n",
    "1) Random forest classification\n",
    "\n",
    "2) Principal component analysis (PCA) + k-nearest neighbours (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b79fc28f-94bc-43f8-8aa6-71315abc8adf",
    "_uuid": "0eb680bcbb6d3649d627e18b7074ed099ed2617e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "sb.set_style(\"dark\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8bcd0340-39fa-6b3c-9b8d-3e9a18f079d1",
    "_uuid": "862ca0e131500f76a0e9d6dc6c6bcb0670d56f57",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use this function in order to evaulate a classifier. It trains on a fraction of the data corresponding to \n",
    "# aplit_ratio, and evaulates on the rest of the data\n",
    "\n",
    "def evaluate_classifier(clf, data, target, split_ratio):\n",
    "    trainX, testX, trainY, testY = train_test_split(data, target, train_size=split_ratio, random_state=0)\n",
    "    clf.fit(trainX, trainY)\n",
    "    return clf.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a747b6e0-cb46-7b34-8f29-092ef30733cb",
    "_uuid": "fd718fac7a53c1a7b1f385f3292b7ff9e65edf7b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test  = pd.read_csv('../input/test.csv')\n",
    "target = train[\"label\"]\n",
    "train = train.drop(\"label\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f121719e-4dfc-1dec-f071-639c7a22a944",
    "_uuid": "673cb0a8a7ee480e9ca3806dfaf70c15ed4c4f2e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot some of the numbers\n",
    "\n",
    "figure(figsize(5,5))\n",
    "for digit_num in range(0,64):\n",
    "    subplot(8,8,digit_num+1)\n",
    "    grid_data = train.iloc[digit_num].as_matrix().reshape(28,28)  # reshape from 1d to 2d pixel array\n",
    "    plt.imshow(grid_data, interpolation = \"none\", cmap = \"bone_r\")\n",
    "    xticks([])\n",
    "    yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bdc541b8-abb8-3507-483c-ae9940dfb9b2",
    "_uuid": "5d692e000ac68c48d599851057def11178f76538",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check performance of random forest classifier, as function of number of estimators \n",
    "# here we only take 1000 data points to train\n",
    "\n",
    "n_estimators_array = np.array([1,5,10,50,100,200,500])\n",
    "n_samples = 10\n",
    "n_grid = len(n_estimators_array)\n",
    "score_array_mu =np.zeros(n_grid)\n",
    "score_array_sigma = np.zeros(n_grid)\n",
    "j=0\n",
    "for n_estimators in n_estimators_array:\n",
    "    score_array=np.zeros(n_samples)\n",
    "    for i in range(0,n_samples):\n",
    "        clf = RandomForestClassifier(n_estimators = n_estimators, n_jobs=1, criterion=\"gini\")\n",
    "        score_array[i] = evaluate_classifier(clf, train.iloc[0:1000], target.iloc[0:1000], 0.8)\n",
    "    score_array_mu[j], score_array_sigma[j] = mean(score_array), std(score_array)\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "26b82ddd-10cc-47d5-0c10-bfef4b60b9f1",
    "_uuid": "2292d9317bd5321bce29dcec682efbe211c4edaf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it looks like the performace saturates around 50-100 estimators\n",
    "\n",
    "figure(figsize(7,3))\n",
    "errorbar(n_estimators_array, score_array_mu, yerr=score_array_sigma, fmt='k.-')\n",
    "xscale(\"log\")\n",
    "xlabel(\"number of estimators\",size = 20)\n",
    "ylabel(\"accuracy\",size = 20)\n",
    "xlim(0.9,600)\n",
    "grid(which=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d22b7510-3c48-5b5e-90bc-6e5d8294fb2d",
    "_uuid": "76a2b02b2bd9208d53a5206f3c3a431d4d97124f"
   },
   "source": [
    "Are there any feature that are particularly important? We can check this using clf.feature_importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d4801307-c5fe-f96f-66a3-fc2db8ac7e1e",
    "_uuid": "73c11ad449508cfe4a0d835554e0906e9283f8b5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(0,10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances\n",
    "\n",
    "figure(figsize(7,3))\n",
    "plot(indices[:],importances[indices[:]],'k.')\n",
    "yscale(\"log\")\n",
    "xlabel(\"feature\",size=20)\n",
    "ylabel(\"importance\",size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3427c56a-7620-22dc-909a-2cf2fa821ff5",
    "_uuid": "e6c6d402640112bc05e2343718f1694ab7e87626"
   },
   "source": [
    "It looks like there are no significantly important features (i.e., pixels) in the original data. Next, let us try to decompose the data using a principal component analysis (PCA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee4f8e1c-ccc3-193c-1763-e1fd1d47852b",
    "_uuid": "6c09c42c7c0d9c687426d57ee43051fa161ef626",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(train)\n",
    "transform = pca.transform(train)\n",
    "\n",
    "figure(figsize(6,5))\n",
    "plt.scatter(transform[:,0],transform[:,1], s=20, c = target, cmap = \"nipy_spectral\", edgecolor = \"None\")\n",
    "plt.colorbar()\n",
    "clim(0,9)\n",
    "\n",
    "xlabel(\"PC1\")\n",
    "ylabel(\"PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c302666-9ba8-18d9-1129-cc45ec68f715",
    "_uuid": "40fcbd448b87ae7baefa9861ab9b0c12c3da8ef1"
   },
   "source": [
    "It is interesting to see how well PCA separates the feature space into visible clusters already for 2 components. Next, let's look at what happens if we increase the number of components in PCA. In particular, we would like to know how many components are needed to capture most of the variance in the data. For this we will use the pca.explained_variance_ratio function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "627aeeab-eddd-1aba-d19a-537e74c453c6",
    "_uuid": "e75f8fc555cc8b24428d93d1373ca7dfc5fad728",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components_array=([1,2,3,4,5,10,20,50,100,200,500])\n",
    "vr = np.zeros(len(n_components_array))\n",
    "i=0;\n",
    "for n_components in n_components_array:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(train)\n",
    "    vr[i] = sum(pca.explained_variance_ratio_)\n",
    "    i=i+1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b737cb19-bcaa-cd24-37b8-6ddb3e3ee684",
    "_uuid": "f21bb7e6291f1a8801414c154069a372a5636395",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize(8,4))\n",
    "plot(n_components_array,vr,'k.-')\n",
    "xscale(\"log\")\n",
    "ylim(9e-2,1.1)\n",
    "yticks(linspace(0.2,1.0,9))\n",
    "xlim(0.9)\n",
    "grid(which=\"both\")\n",
    "xlabel(\"number of PCA components\",size=20)\n",
    "ylabel(\"variance ratio\",size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "695af730-5d79-7c8c-97cb-a1fb2f8068bc",
    "_uuid": "0284dcb748852ca66e21b90aca79243a1325382e"
   },
   "source": [
    "We see that ~100 PCA components are needed to capture ~90% of the variance in the data. This seems a lot of components. Maybe the more important question is: How good is our prediction as a function of number of components? Let's look at this next. We will train a kNN classifier on the PCA output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc65a69f-7073-b937-d975-88c98b5c66d2",
    "_uuid": "d23f41d9087b41a8264417933eff0c152a44d07c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "n_components_array=([1,2,3,4,5,10,20,50,100,200,500])\n",
    "score_array = np.zeros(len(n_components_array))\n",
    "i=0\n",
    "\n",
    "for n_components in n_components_array:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(train)\n",
    "    transform = pca.transform(train.iloc[0:1000])\n",
    "    score_array[i] = evaluate_classifier(clf, transform, target.iloc[0:1000], 0.8)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68e04bb4-c783-4cba-cd6e-b3db3a5a279a",
    "_uuid": "aaee14895d5c00589f962e65821dc96b4eb66329",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize(8,4))\n",
    "plot(n_components_array,score_array,'k.-')\n",
    "xscale('log')\n",
    "xlabel(\"number of PCA components\", size=20)\n",
    "ylabel(\"accuracy\", size=20)\n",
    "grid(which=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73193bdb-b039-e0d6-6db4-689799382f53",
    "_uuid": "0834d9ec80d4fe181f17179d800e00b9dae17692"
   },
   "source": [
    "The accuracy seems to saturate at ~90% (roughly matching the performance of the random forest classifier) for >~20 PCA components. In fact, the accuracy even seems to drop for much larger numbers, even though a larger number of PCA components captures more of the variance in the data, as seen in the plot above. The drop in accuracy is probably due to overfitting.\n",
    "\n",
    "Finally, we will train on the whole training set and prepare a submit file for the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ed495d33-8254-4058-ef27-dae03ccf4d30",
    "_uuid": "ed0c51f4fa678993f14c684b0c80830e2c6a8ba9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA + kNN\n",
    "    \n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(train)\n",
    "transform_train = pca.transform(train)\n",
    "transform_test = pca.transform(test)\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(transform_train, target)\n",
    "results=clf.predict(transform_test)\n",
    "\n",
    "# prepare submit file\n",
    "\n",
    "np.savetxt('results.csv', \n",
    "           np.c_[range(1,len(test)+1),results], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')\n",
    "\n",
    "# Kaggle score 0.97343"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2aea948-1727-e33c-040a-57e1e749ccc8",
    "_uuid": "511c2932ef5ba79e9d06cfc41cdaa05a0b61806a"
   },
   "source": [
    "This gives a score on Kaggle of 0.97343 - not too bad! Using the random forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d441f1ae-7ba7-9001-23b4-c72533568aec",
    "_uuid": "3a9ff3ddc63b78e357682aba064f9d6d2845f3b8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest classification\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100, n_jobs=1, criterion=\"gini\")\n",
    "clf.fit(train, target)\n",
    "results=clf.predict(test)\n",
    "\n",
    "# prepare submit file\n",
    "\n",
    "np.savetxt('results.csv', \n",
    "           np.c_[range(1,len(test)+1),results], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')\n",
    "\n",
    "# Kaggle score ~0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d476c2d-e855-0ca5-c444-649ba49af635",
    "_uuid": "b50d7c4d1abab3ce4aadad7194a9a88554da338b"
   },
   "source": [
    "This gives a slightly worse score (0.96).\n",
    "\n",
    "Any feedback on my analysis is more than welcome!"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
