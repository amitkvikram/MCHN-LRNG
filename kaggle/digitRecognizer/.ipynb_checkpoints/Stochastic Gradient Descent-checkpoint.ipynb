{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Digit Recognizer using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "raw_data = open(\"train.csv\",'rt')\n",
    "data = np.loadtxt(raw_data, delimiter = ',', skiprows =1)\n",
    "\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data\n",
    "    - training data : 60%\n",
    "    - test data: 20%\n",
    "    - cross validation data: 20%\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-d1a18385d4df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#Adding polynomial features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0madd_x0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;31m# allocate output data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mXP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_output_features_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         combinations = self._combinations(n_features, self.degree,\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def normalize(sigma2, mean2, X):\n",
    "    X = (X-mean2)/sigma2\n",
    "    return X\n",
    "\n",
    "def add_x0(X):\n",
    "    return np.column_stack((np.zeros(X.shape[0],), X))\n",
    "\n",
    "np.random.shuffle(data)\n",
    "np.random.shuffle(data)\n",
    "X_train1 = data[:33600,1:]\n",
    "variance = (np.var(data[:,1:], axis =0)>100)\n",
    "X_train = X_train1[:,variance]\n",
    "\n",
    "sigma = np.std(X_train, axis =0)\n",
    "mean = np.mean(X_train, axis =0)\n",
    "sigma1 = np.std(X_train1, axis =0)\n",
    "mean1 = np.mean(X_train1, axis =0)\n",
    "\n",
    "X_train2 = X_train\n",
    "X_train1 = normalize(sigma1, mean1 , X_train1)      #without removing columns with less variance\n",
    "X_train = normalize(sigma , mean , X_train)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(np.var(X_train, axis =0))\n",
    "\n",
    "X_cv1 = data[25200:33600, 1:]\n",
    "X_cv = X_cv1[:,variance]\n",
    "X_cv2 = X_cv\n",
    "X_cv = normalize(sigma , mean , X_cv)\n",
    "X_cv1 = normalize(sigma1 , mean1 , X_cv1)\n",
    "\n",
    "X_test1 = data[33600:, 1:]\n",
    "X_test = X_test1[:,variance]\n",
    "X_test = normalize(sigma , mean , X_test)\n",
    "X_test1 = normalize(sigma1 , mean1 , X_test1)\n",
    "\n",
    "#Adding x0\n",
    "X_train1= add_x0(X_train1)\n",
    "X_cv1= add_x0(X_cv1)\n",
    "X_cv= add_x0(X_cv)\n",
    "X_test1 = add_x0(X_test1)\n",
    "###########################\n",
    "\n",
    "\n",
    "y_train = data[:33600, 0]\n",
    "y_cv = data[25200:33600, 0]\n",
    "y_test = data[33600:, 0]\n",
    "\n",
    "\n",
    "# print(X_train.shape, X_cv.shape, X_test.shape)\n",
    "# print(y_train.shape, y_cv.shape, y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Adding polynomial features\n",
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "X_train= add_x0(X_train)\n",
    "X_test = add_x0(X_test)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((X_train.shape[1],10)) \n",
    "def cost(X, y, theta):\n",
    "    m = X.shape[0]\n",
    "    J = (1/(2*m))*np.sum(np.square(np.dot(X,theta) -y))\n",
    "\n",
    "    return J;\n",
    "\n",
    "def hypothesis(x_tmp, theta1):\n",
    "    h = np.dot(np.transpose(x_tmp), theta1)\n",
    "    h_temp = h[0,0]\n",
    "    h_temp = 1/(1+np.exp(-h_temp))\n",
    "#     print(h_temp)\n",
    "    return h_temp\n",
    "\n",
    "def stochastic_gradientDescent(X, y, theta):\n",
    "    m = X.shape[0]\n",
    "    alpha = 0.0001\n",
    "    for k in range(0,10):\n",
    "        print(k)\n",
    "        for i in range(1,10):\n",
    "            for j in range(1,m):\n",
    "                x_tmp = X[i, :]\n",
    "                x_tmp = x_tmp.reshape(x_tmp.shape[0], 1)\n",
    "                y_tmp = (y[i] == k).astype(np.float64)\n",
    "                theta1 = theta[:,k].reshape(theta[:,k].shape[0],1)\n",
    "                theta[:,k] = (theta1 -(np.dot((alpha*x_tmp),(hypothesis(x_tmp, theta1) - y_tmp)))).reshape(theta1.shape[0], ) \n",
    "#                 print(\"theta=\",theta[:,k])\n",
    "#             print(cost(x_tmp, y_tmp, theta[:,k]))\n",
    "\n",
    "stochastic_gradientDescent(X_train, y_train, theta) \n",
    "print(theta)\n",
    "np.savetxt(\"del.txt\", theta, delimiter =',', newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906785714286\n",
      "{'random_state': None, 'warm_start': False, 'shuffle': True, 'power_t': 0.5, 'learning_rate': 'optimal', 'alpha': 0.0001, 'class_weight': None, 'tol': None, 'average': False, 'penalty': 'l2', 'l1_ratio': 0.15, 'fit_intercept': True, 'loss': 'hinge', 'eta0': 0.0, 'verbose': 0, 'epsilon': 0.1, 'max_iter': 5, 'n_iter': None, 'n_jobs': 1}\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.SGDClassifier(alpha = 0.0001)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "theta = np.zeros((X_train2.shape[1]+1,10)) \n",
    "\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "theta = np.column_stack((clf.intercept_, clf.coef_))\n",
    "# print(theta.T)\n",
    "print(clf.get_params(deep = True))\n",
    "\n",
    "np.savetxt(\"del.txt\", theta.T, delimiter =',', newline='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937678571429 0.90880952381\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LogisticRegression(solver='lbfgs')\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "score1 = regr.score(X_train, y_train)\n",
    "score = regr.score(X_test, y_test)\n",
    "print(score1, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
